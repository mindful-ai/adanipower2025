{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "boiler_input = pd.read_csv(\"boilerinput.csv\")\n",
    "boiler_output = pd.read_csv(\"boileroutput.csv\")\n",
    "coal_data = pd.read_csv(\"coaldf.csv\")\n",
    "o2_data = pd.read_csv(\"O2_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the timestamps\n",
    "boiler_input['dates'] = pd.to_datetime(boiler_input['dates'])\n",
    "boiler_output['dates'] = pd.to_datetime(boiler_output['dates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge datasets based on timestamps\n",
    "data = pd.merge_asof(boiler_input.sort_values('dates'), boiler_output.sort_values('dates'), on='dates')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant features for states\n",
    "states = list(zip(\n",
    "    data[\"PRIMARY AIR FLOW\"].round(1),\n",
    "    data[\"SECOND AIR FLOW TOTAL\"].round(1),\n",
    "    data[\"Main Steam Flow\"].round(1)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define actions\n",
    "actions = [\"increase_primary\", \"decrease_primary\", \"increase_secondary\", \"decrease_secondary\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Q-table\n",
    "# Q_table = {state: {action: 0 for action in actions} for state in states}\n",
    "# Initialize Q-table dynamically\n",
    "Q_table = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.9  # Discount factor\n",
    "epsilon = 0.1  # Exploration rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated environment function\n",
    "def get_next_state(state, action):\n",
    "    primary, secondary, steam = state\n",
    "\n",
    "    if action == \"increase_primary\":\n",
    "        primary = min(primary + 2, data[\"PRIMARY AIR FLOW\"].max())  \n",
    "    elif action == \"decrease_primary\":\n",
    "        primary = max(primary - 2, data[\"PRIMARY AIR FLOW\"].min())  \n",
    "    elif action == \"increase_secondary\":\n",
    "        secondary = min(secondary + 2, data[\"SECOND AIR FLOW TOTAL\"].max())  \n",
    "    elif action == \"decrease_secondary\":\n",
    "        secondary = max(secondary - 2, data[\"SECOND AIR FLOW TOTAL\"].min())\n",
    "\n",
    "    return (primary, secondary, steam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward function based on boiler output\n",
    "def get_reward(state):\n",
    "    primary, secondary, steam = state\n",
    "    row = data[(data[\"PRIMARY AIR FLOW\"] == primary) & (data[\"SECOND AIR FLOW TOTAL\"] == secondary)]\n",
    "    \n",
    "    if row.empty:\n",
    "        return -1  # Default penalty for unknown state\n",
    "    \n",
    "    efficiency = row[\"Boiler Efficiency\"].values[0]\n",
    "    total_loss = row[\"Loss Total\"].values[0]\n",
    "\n",
    "    reward = 0\n",
    "    if efficiency >= 90:\n",
    "        reward += 10  # Good efficiency\n",
    "    elif efficiency < 85:\n",
    "        reward -= 5  # Poor efficiency\n",
    "\n",
    "    if total_loss > 12:\n",
    "        reward -= 10  # High losses\n",
    "    elif total_loss <= 10:\n",
    "        reward += 5  # Low losses\n",
    "\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for episode in range(1000):\n",
    "    state = random.choice(states)\n",
    "\n",
    "    # Ensure the state is in Q-table\n",
    "    if state not in Q_table:\n",
    "        Q_table[state] = {action: 0 for action in actions}\n",
    "\n",
    "    for _ in range(50):  \n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = random.choice(actions)  # Explore\n",
    "        else:\n",
    "            action = max(Q_table[state], key=Q_table[state].get)  # Exploit\n",
    "\n",
    "        next_state = get_next_state(state, action)\n",
    "        reward = get_reward(next_state)\n",
    "\n",
    "        # Ensure next_state exists in Q_table\n",
    "        if next_state not in Q_table:\n",
    "            Q_table[next_state] = {action: 0 for action in actions}\n",
    "\n",
    "        # Q-learning update\n",
    "        best_next_action = max(Q_table[next_state], key=Q_table[next_state].get)\n",
    "        Q_table[state][action] += alpha * (reward + gamma * Q_table[next_state][best_next_action] - Q_table[state][action])\n",
    "\n",
    "        state = next_state  # Move to next state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: (196.1, 525.1, 605.5), Actions: {'increase_primary': -0.1, 'decrease_primary': 0, 'increase_secondary': 0, 'decrease_secondary': 0}\n",
      "State: (198.1, 525.1, 605.5), Actions: {'increase_primary': -0.1, 'decrease_primary': 0, 'increase_secondary': 0, 'decrease_secondary': 0}\n",
      "State: (200.1, 525.1, 605.5), Actions: {'increase_primary': 0, 'decrease_primary': 0, 'increase_secondary': 0, 'decrease_secondary': 0}\n"
     ]
    }
   ],
   "source": [
    "# Print trained Q-table (first 10 entries for simplicity)\n",
    "for state, actions in list(Q_table.items())[-3:]:\n",
    "    print(f\"State: {state}, Actions: {actions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State: (178.1, 527.1, 605.5)\n",
      "Agent Suggests Action: increase_primary\n",
      "New State After Action: (180.1, 527.1, 605.5)\n",
      "New Reward (Efficiency - Loss): -1\n"
     ]
    }
   ],
   "source": [
    "# Simulate a real-time scenario\n",
    "# initial_state = (229.0, 522.2, 537.3)\n",
    "# initial_state = (231.2, 524.1, 547.2)\n",
    "initial_state = (178.1, 527.1, 605.5)\n",
    "print(f\"Initial State: {initial_state}\")\n",
    "\n",
    "# Choose the best action from Q-table\n",
    "best_action = max(Q_table[initial_state], key=Q_table[initial_state].get)\n",
    "print(f\"Agent Suggests Action: {best_action}\")\n",
    "\n",
    "# Apply action and get new state\n",
    "new_state = get_next_state(initial_state, best_action)\n",
    "print(f\"New State After Action: {new_state}\")\n",
    "\n",
    "# Compute the reward\n",
    "reward = get_reward(new_state)\n",
    "print(f\"New Reward (Efficiency - Loss): {reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
